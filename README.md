# üáªüá≥ Vietnamese News Classification with PhoBERT v2

A Vietnamese news article classifier fine-tuned on **PhoBERT v2** (`vinai/phobert-base-v2`) that categorises articles into **10 topics** with a weighted F1-score of **91.27%** on the held-out test set. This project is an upgraded continuation of my earlier NLP classification work during my Bachelor's studies.

---

## üß† Overview

This project fine-tunes [PhoBERT-base-v2](https://huggingface.co/vinai/phobert-base-v2) ‚Äî a RoBERTa-based language model pre-trained specifically on Vietnamese text ‚Äî for multi-class news article classification.

**Pipeline at a glance:**

1. Raw Vietnamese news articles are cleaned and tokenised (via `pyvi`) into preprocessed category files under `clean-data/`.
2. `prepare_data.py` maps the cleaned category files into labelled Pandas DataFrames and performs a stratified train / val / test split.
3. `train.py` fine-tunes PhoBERT-base-v2 using a standard AdamW + linear warmup schedule, with early stopping and best-checkpoint saving.

---

## üóÇ Categories

The model classifies articles into the following 10 Vietnamese news topics:

| ID | Category (Vietnamese) | Translation |
|----|-----------------------|-------------|
| 0 | Chinh tri Xa hoi | Politics & Society |
| 1 | Doi song | Lifestyle |
| 2 | Khoa hoc | Science |
| 3 | Kinh doanh | Business |
| 4 | Phap luat | Law |
| 5 | Suc khoe | Health |
| 6 | The gioi | World |
| 7 | The thao | Sports |
| 8 | Van hoa | Culture |
| 9 | Vi tinh | Technology |

---

## üèó Project Structure

```
vietnamese-news-classification-phobert/
‚îú‚îÄ‚îÄ Train_Full/                  # [Ignored] Raw source for Validation + Test data (Available on request)
‚îú‚îÄ‚îÄ Test_Full/                   # [Ignored] Raw source for Training data*
‚îú‚îÄ‚îÄ clean-data/                  # [Ignored] Preprocessed datasets 
‚îÇ   ‚îú‚îÄ‚îÄ train/                   # Processed Validation + Test data (~33k articles)
‚îÇ   ‚îî‚îÄ‚îÄ test/                    # Processed Training data (~50k articles)*
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py         # Vietnamese text normalisation, segmentation, TF-IDF utilities
‚îÇ   ‚îú‚îÄ‚îÄ prepare_data.py          # Dataset loader and train/val/test splitter
‚îÇ   ‚îú‚îÄ‚îÄ train.py                 # Fine-tuning script (PhoBERT + callbacks)
‚îÇ   ‚îî‚îÄ‚îÄ main.py                  # Inference helpers + quick CLI test runner
‚îú‚îÄ‚îÄ phobert-v2/                  # All training outputs (Auto-generated by train.py)
‚îÇ   ‚îú‚îÄ‚îÄ best_model/              # Best checkpoint (saved by val loss)
‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors        # [Ignored] Final model weights
‚îÇ   ‚îú‚îÄ‚îÄ config.json              # Model config
‚îÇ   ‚îú‚îÄ‚îÄ vocab.txt / bpe.codes    # Tokenizer vocabulary files
‚îÇ   ‚îú‚îÄ‚îÄ label_map.json           # Category ‚Üí integer label mapping
‚îÇ   ‚îú‚îÄ‚îÄ confusion_matrix.png     # Per-class confusion matrix on the test set
‚îÇ   ‚îú‚îÄ‚îÄ training_history.csv     # Per-epoch metrics (loss, acc, precision, recall, F1)
‚îÇ   ‚îî‚îÄ‚îÄ training_log.txt         # Full training log      
‚îú‚îÄ‚îÄ dictionary.txt               # Filtered vocabulary built from training data
‚îú‚îÄ‚îÄ stopword.txt                 # Vietnamese stopword list
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

**Note on Data Split:** To maximize the model's learning capacity, the folders have been swapped: clean-data/test/ is used as the primary Training source (approx. 50k articles), while clean-data/train/ serves as the source for Validation and Testing (approx. 33k articles).

---

## üìä Results

Training was run for **6 epochs** (early stopping triggered after epoch 6 due to no val-loss improvement for 3 consecutive epochs). Each epoch took approximately **21.6 minutes** on a CUDA GPU.

### üß™ Training History

| Epoch | Train Loss | Train Acc | Val Loss | Val Acc | Val F1 |
|-------|-----------|-----------|----------|---------|--------|
| 1 | 1.1110 | 75.66% | 0.4572 | 88.88% | 88.91% |
| 2 | 0.3132 | 91.95% | 0.3292 | 90.30% | 90.34% |
| 3 | 0.2124 | 93.92% | 0.2952 | 91.44% | 91.41% |
| 4 | 0.1635 | 95.32% | 0.3163 | 91.62% | 91.63% |
| 5 | 0.1320 | 96.39% | 0.3247 | 92.19% | 92.18% |
| 6 | 0.1053 | 97.19% | 0.3546 | 92.12% | 92.10% |

### üßæ Test Set Performance (Best Checkpoint ‚Äî Epoch 3)

| Metric | Score |
|--------|-------|
| Loss | 0.3053 |
| Accuracy | **91.32%** |
| Weighted Precision | **91.38%** |
| Weighted Recall | **91.32%** |
| Weighted F1 | **91.27%** |

> The best model checkpoint was saved at **epoch 3** (lowest val loss: 0.2952). The confusion matrix is saved at `phobert-v2/confusion_matrix.png`.

---

## ‚öôÔ∏è Requirements

- Python 3.10+
- CUDA-capable GPU

Install all dependencies:

```bash
pip install -r requirements.txt
```

---

## üöÄ Setup

```bash
git clone https://github.com/longle0702/vietnamese-news-classification-phobert.git
cd vietnamese-news-classification-phobert

python -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt
```

---

## üõ† Usage

### 1. Data Preprocessing

Raw articles (one file per article, encoded UTF-16) should be placed inside category-named subdirectories under `Train_Full/` and `Test_Full/`. Running the preprocessing pipeline will:

- Normalise Unicode and Vietnamese tone marks
- Segment words via `pyvi`
- Remove stopwords
- Write one cleaned `.txt` per category under `clean-data/train/` and `clean-data/test/`

```bash
python src/preprocessing.py
```

### 2. Training

```bash
python src/train.py
```

This will:

1. Load and label data from `clean-data/test/` (training set, ~50 k articles) and split `clean-data/train/` 50/50 into validation and test sets (~16.9 k each).
2. Tokenise all splits using the PhoBERT v2 tokenizer (downloaded automatically from the Hugging Face Hub on first run).
3. Fine-tune `vinai/phobert-base-v2` with AdamW, a linear warmup scheduler, gradient clipping, and early stopping.
4. Save the best checkpoint to `phobert-v2/best_model/`, the final model to `phobert-v2/`, and the training history to `phobert-v2/training_history.csv`.
5. Evaluate the best checkpoint on the test set and save the confusion matrix to `phobert-v2/confusion_matrix.png`.

### 3. Training Arguments

All hyperparameters can be overridden via command-line flags:

| Flag | Default | Description |
|------|---------|-------------|
| `--epochs` | `10` | Maximum number of training epochs |
| `--batch_size` | `32` | Training batch size |
| `--eval_batch_size` | `64` | Evaluation batch size |
| `--max_len` | `256` | Maximum token sequence length |
| `--lr` | `1e-5` | AdamW learning rate |
| `--warmup_ratio` | `0.1` | Fraction of total steps used for linear warmup |
| `--patience` | `3` | Early stopping patience (epochs without val-loss improvement) |

**Example ‚Äî shorter run with a smaller batch:**

```bash
python src/train.py --epochs 3 --batch_size 16
```

### 4. Run Inference

`main.py` exposes three importable helpers (`load_label_map`, `load_model`, `predict`) and also doubles as a quick CLI test runner.

**Interactive mode** ‚Äî prompts you to type the article text:

```bash
python src/main.py
# Enter news text: <type or paste your article here>
```

**CLI mode** ‚Äî pass the text directly as arguments:

```bash
python src/main.py ƒê·ªôi tuy·ªÉn Vi·ªát Nam th·∫Øng 2-0 tr∆∞·ªõc Th√°i Lan t·∫°i SEA Games.
```

> **Tip:** For long articles or text containing `"` (double-quote) characters, use the interactive mode or pipe from a file:
> ```bash
> cat article.txt | python src/main.py
> ```

**Sample output:**

```
Input : Bjorkan ƒë·ªôt ng·ªôt r·ªùi ƒëi, Knutsen t·ª´ v·ªã tr√≠ tr·ª£ l√Ω HLV ƒë∆∞·ª£c b·ªï nhi·ªám l√†m thuy·ªÅn tr∆∞·ªüng...
====================================================================================================
  [1] The thao                   98.84%  |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë|
  [2] Doi song                    0.24%  |‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë|
  [3] Van hoa                     0.19%  |‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë|
====================================================================================================
‚Üí The thao  (98.84%)
```

The model always shows the **top-3 predictions** with a confidence bar. The best checkpoint (`phobert-v2/best_model/`) is loaded automatically.

---

## üì¶ Output Artifacts

| Path | Description |
|------|-------------|
| `phobert-v2/best_model/` | Best checkpoint (by lowest validation loss), loadable with `AutoModelForSequenceClassification.from_pretrained()` |
| `phobert-v2/model.safetensors` | Final model weights (after all epochs) |
| `phobert-v2/label_map.json` | JSON mapping of category name ‚Üí integer label |
| `phobert-v2/training_history.csv` | CSV with per-epoch `loss`, `acc`, `precision`, `recall`, `f1` for train/val/test splits |
| `phobert-v2/training_log.txt` | Full timestamped training log |
| `phobert-v2/confusion_matrix.png` | Confusion matrix of the best model on the test set |

## ‚ù§Ô∏è Acknowledgements
I would like to thank my former teammates who contributed to my earlier NLP projects during my Bachelor's journey. Your collaboration, discussions, and support helped lay the foundation for this work.
